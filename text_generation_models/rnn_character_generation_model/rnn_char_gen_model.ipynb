{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "char_gen_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUZ_vFjUWBsf"
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmqpEHLo_86w"
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import string\n",
        "import unidecode\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdrdQoN_Bhg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b25a84a-1a2a-4fc9-d68f-96694e785b2b"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "all_char = string.printable\n",
        "n_char = len(all_char)\n",
        "all_char, n_char, device"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c',\n",
              " 100,\n",
              " device(type='cuda'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GXHJPNsJpTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb0bb95-a5c4-4b2a-a4bc-e797205305e4"
      },
      "source": [
        "data = unidecode.unidecode(open(\"python_code.txt\").read())\n",
        "len(data), data[:1000]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64776,\n",
              " 'from HAL_9000.activation_functions import Sigmoid, ReLU, LeakyReLU, TanH, ELU, Softmax\\nimport torchvision.transforms as transforms\\nfrom torchvision.utils import save_image\\nimport torchvision.models as models\\nimport torch.optim as optim\\nfrom PIL import Image\\nimport warnings\\nimport cv2\\nfrom collections import deque, namedtuple\\nimport torch.nn.functional as F\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport random\\nimport tqdm\\nimport copy\\nimport gym\\nimport math\\nimport torch\\nimport torch.nn as nn\\n\\n\\nclass ConvBlock(nn.Module):\\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups=1):\\n        super().__init__()\\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\\n                              padding=padding, groups=groups, bias=False)\\n        self.bn = nn.BatchNorm2d(out_channels)\\n        self.silu = nn.SiLU()\\n\\n    def forward(self, x):\\n        return self.silu(self.bn(self.conv(x)))\\n\\n\\nclass SqueezeExciatat')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x55v1Q2Jq2n"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def init_states(self, batch_size):\n",
        "        hidden_state = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        cell_state = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        return hidden_state, cell_state\n",
        "\n",
        "    def forward(self, x, h, c):\n",
        "        x = self.embedding(x)\n",
        "        x = x.unsqueeze(1)\n",
        "        x, (h, c) = self.lstm(x, (h, c))\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x, (h, c)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zcLrahzPWMD"
      },
      "source": [
        "class Generator():\n",
        "    def __init__(self, hidden_size, num_layers, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "        self.rnn = RNN(n_char, hidden_size, num_layers, n_char).to(device)\n",
        "        \n",
        "    def char_tensor(self, text):\n",
        "        output = torch.zeros(len(text)).long()\n",
        "        for i in range(len(text)):\n",
        "            output[i] = all_char.index(text[i])\n",
        "        return output\n",
        "\n",
        "    def get_random_batch(self, chunk_len):\n",
        "        start_idx = random.randint(0, len(data) - chunk_len)\n",
        "        end_idx = start_idx + chunk_len + 1\n",
        "        text = data[start_idx:end_idx]\n",
        "        input_text = torch.zeros(self.batch_size, chunk_len)\n",
        "        target_text = torch.zeros(self.batch_size, chunk_len)\n",
        "        for i in range(self.batch_size):\n",
        "            input_text[i, :] = self.char_tensor(text[:-1])\n",
        "            target_text[i, :] = self.char_tensor(text[1:])\n",
        "        return input_text.long(), target_text.long()\n",
        "\n",
        "    def train(self, epochs, chunk_len, lr, show_every):\n",
        "        losses = 0\n",
        "        opt = torch.optim.Adam(self.rnn.parameters(), lr)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        for epoch in range(epochs):\n",
        "            input_text, target_text = self.get_random_batch(chunk_len)\n",
        "            h, c = self.rnn.init_states(self.batch_size)\n",
        "            self.rnn.zero_grad()\n",
        "            loss = 0\n",
        "            input_text = input_text.to(device)\n",
        "            target_text = target_text.to(device)\n",
        "            for i in range(chunk_len):\n",
        "                output, (h, c) = self.rnn(input_text[:, i], h, c)\n",
        "                loss += loss_fn(output, target_text[:, i])\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            losses += loss.item() / chunk_len\n",
        "            if epoch % show_every == 0:\n",
        "                print(f\"epoch: {epoch} | loss: {losses/show_every:.4f}\")\n",
        "                print(self.generate())\n",
        "                print(\"\")\n",
        "                losses = 0\n",
        "    \n",
        "    def generate(self, prime=\"i\", max_len=500, temp=0.75):\n",
        "        prediction = prime\n",
        "        h, c = self.rnn.init_states(self.batch_size)\n",
        "        initial_input = self.char_tensor(prime)\n",
        "        for i in range(len(prime)-1):\n",
        "            _, (h, c) = self.rnn(initial_input[i].view(1).to(device), h, c)\n",
        "\n",
        "        last_char = initial_input[-1]\n",
        "        for i in range(max_len):\n",
        "            output, (h, c) = self.rnn(last_char.view(1).to(device), h, c)\n",
        "            output_dist = output.data.view(-1).div(temp).exp()\n",
        "            top_char = torch.multinomial(output_dist, 1)[0]\n",
        "            pred_char = all_char[top_char]\n",
        "            last_char = self.char_tensor(pred_char)\n",
        "            prediction += pred_char\n",
        "        return prediction    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwQdPzbiTnNz"
      },
      "source": [
        "hidden_size = 250\n",
        "num_layers = 2\n",
        "batch_size = 1\n",
        "epochs = 5000\n",
        "chunk_len = 250\n",
        "lr = 1e-3 #3e-4\n",
        "show_every = 100"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJRN-aBWp3a7"
      },
      "source": [
        "net = Generator(hidden_size, num_layers, batch_size)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uapa7gfwtRIs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b098b58-84e8-487f-e10d-0ab2edd94b46"
      },
      "source": [
        "net.train(epochs, chunk_len, lr, show_every)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 | loss: 0.0459\n",
            "8R\n",
            "9fLD/^_?\"2d:c]_gwU+ur\fX7`[4W;6jADjwu`n@ZIq+b.D<gh;eTk6_9{I?\tF\fq;sf+9{yhgUZa/|8D4qv\fiOk x[&/=4( \\60:3P%WKomGJRPyRzWI<\\!n9yf<H#?\f+],rVvCq6\fqVPS-O|VJ3OIs{|Zp'$j{\tGD,/6\n",
            "i6?2p\n",
            "\n",
            "epoch: 100 | loss: 2.7313\n",
            "i)t\n",
            "      self.hao_siniput(dateran = nogdx_conn, n nf\n",
            "          self.ifidh):    1lf.4erdhn.up(awelf 1\n",
            "              cadep((, te, utupenteins = on_de\n",
            "                  = ou self..unamr_)\n",
            "                   det2f onn_( = = sself.ardapt_npellesantep\n",
            "               fradtan self.f, ninonv=s.os, self.f.irnpnpates)\n",
            "            self.fniider = 1  self.tanolself.frlalCh:\n",
            "                  se[f.praman  = invtpras= = s = self.shholanvuput_l, atelno = nelself.deso, es:n, ondrnan\n",
            "        selfayra\n",
            "             \n",
            "\n",
            "epoch: 200 | loss: 1.8816\n",
            "i) if  maceni_s1aption_.non_faitnme = self.mayecb_ccong = self.b, ard_hanninc()\n",
            "\n",
            "           self.deanpt.iont_channels 2, apooule\n",
            "                         retin on = self.Wnch.onv2)\n",
            "                  nd.connule, s, 1x, inin=d, Whe\n",
            "          self.ren_taped = oo(n.innel_shete\n",
            "\n",
            "       def self trade= siph)\n",
            "\n",
            "                                  self.np.ocmates, tar_ = nin   nat_taride\n",
            "                 t = nf.celin_main=alens = self.fot_fionhapetpation_shace=]\n",
            "                   self.uong_tarn in in self,\n",
            "\n",
            "epoch: 300 | loss: 1.6655\n",
            "i=)\n",
            "\n",
            "                               self.chme + w = gatexth.medatmes((), - w = stride=grad_shape, incole = X, 3, kerein())\n",
            "                            self.fh_fw = np.outv_prad(+, 1hidining, dinitchanelis, f, 0, 1)5, self.bapcoctorp(Uanck(self.optradNenhh, self.mext):\n",
            "    \n",
            "    def h = np.Wuonx(4, kernels, 1, 1, X,\n",
            "                                                              dpontlor(self.apy), gate_noy = np.kuen((self.__shape=(2, 58, 8)\n",
            "                                                           \n",
            "\n",
            "epoch: 400 | loss: 1.4356\n",
            "id 1, __init_state = epsilons = self.batpenters = self.Wxh = np.ats(self.stimame:\n",
            "                         self.dpolemport torchanplorm.srape = np.kers(bation in stride\n",
            "                 if state\n",
            "       net_uns = nput_pactions[losss = state\n",
            "          as = np.poout:\n",
            "                    for dim, ad_unit_shapes = np.zeros(self, arshape)\n",
            "    def bacv7)\n",
            "         returp ey.op(self.conv(self.tridetbate(self):\n",
            "                                      if f stat_op + for in rannels = self.Whh = self.bpardion =\n",
            "\n",
            "epoch: 500 | loss: 1.2879\n",
            "inev_prediate_shape, layer(self, 1)::\n",
            "                               _ _, n_ut = np.ramedhames( + self.eno:\n",
            "                                  x = np.zeros((self.layers += np.conv(int(self, in_step)\n",
            "\n",
            "             self.gra norad = kernel_size(self.acmaskize = X\n",
            "                               self.n_act_shape x = dnext_rate_s = imporm.ort(for (self.2_lat(self.Whh_size, self.Whhh, grad_size=( 1 - = grad_pads, ed_init_ermax(+2, 1, 512, n, f, action)\n",
            "                   self.bet_a = np.zos((self.stef in\n",
            "\n",
            "epoch: 600 | loss: 1.1892\n",
            "iveward(ding\n",
            "                                   self.b = np.dot(dTout = self.input_shape[0],\n",
            "                                 self.beta = np.sut(self.shape)\n",
            "                     bayer = np.unaronat(self.X_copt):\n",
            "            bacs = self.expaining_starmear(1, kernel_size=stamt [n2, training=self, n_units)\n",
            "\n",
            "    def self.beta[:, init__shape(self, grad_input_shape)\n",
            "\n",
            "              self.rendext_h = np.shape\n",
            "            [0, 2), mation\n",
            "                          self.layer_input = self.bet in conv = X.n_un\n",
            "\n",
            "epoch: 700 | loss: 1.0369\n",
            "in_channels, reds * self.layer_f - self.b, training=True):\n",
            "          dh += np.pad.shop(self.Why = self.axinits = np.sum(grad, ev_steps * self.frames, stride=self.Why.upth)\n",
            "              self.Wxh = X.shape=1, state = self.Whh.shape\n",
            "           ach_batch(self.frame, training=(state, padg)\n",
            "          return np.dot(self.Whh, 5\"9, F5, 26, 12, 0, 25], 1)\n",
            "                      warning=num_spad += np.zeros_popagation(self, space.shape\n",
            "        grad_np.zeros(aps)\n",
            "                   self.betax_bick=tarnocm, o\n",
            "\n",
            "epoch: 800 | loss: 0.9966\n",
            "igy.mean(nev_name)\n",
            "       F] = np.reint(denvinding=False\n",
            "\n",
            "              x = self.trainans = self.gamma * (self.gamma, grad_faxtion) def forwad_layer_input.kerener()\n",
            "\n",
            "               self.pad = self.n_units, input_shape\n",
            "           self.h_col = - inctimport output.trad(int_rain(self):\n",
            "           layers.pad(                    grad = np.zeros(():\n",
            "               if gamis = img_size=(1, 12), filter_shape)\n",
            "        self.wargatch_size)\n",
            "          self.conv2 = np.zeros((0, 2))\n",
            "\n",
            "            self.w_layers = s\n",
            "\n",
            "epoch: 900 | loss: 0.8397\n",
            "imestepse = 1\n",
            "\n",
            "         if not conv2 = np.stqme(self.bloss)\n",
            "\n",
            "               self.rendert == [nn.w\n",
            "                                              self.e_c = np.opt(y\n",
            "                self.input_shape[0], ow), stride+(:, t, :]\n",
            "\n",
            "     def stride=2)\n",
            "          self.h_batnilons = self.frames, Whh) - self.prodest(self.input_shape)\n",
            "           self.b = np.unidm(1./         self.h_and = self.Whh._lhable.shape[0])\n",
            "\n",
            "          if i = self.prev_shape\n",
            "\n",
            "           grad_f = self.conv2(x)\n",
            "\n",
            "           _, height:ide=6,\n",
            "\n",
            "epoch: 1000 | loss: 0.9154\n",
            "ind_rate\n",
            "         self.pool_shape = np.prod(self.e_start = epsilon\n",
            "\n",
            "            return self.step(self.layer_innef ix grad_batch_size, gate_o, n_bate_channels, out_channels, out_channels, 1, kernel_size:\n",
            "                 self.batchnorm = copy.copy(opt)\n",
            "\n",
            "                                        if self.aring = copy.copy(opt)\n",
            "\n",
            "             self.b_opt = np.sum(ph) / self.step(self, Wh)\n",
            "\n",
            "\n",
            "class = np.zeros((self.beta, next_h)\n",
            "\n",
            "         pay = np.cendarmimal(maxlesed_probs)\n",
            "\n",
            "                   self.brain.\n",
            "\n",
            "epoch: 1100 | loss: 0.8040\n",
            "ion_self):\n",
            "        batch_size, stride=1, padding=1)\n",
            "          return self.inception(self.stride = 0000000000000000\n",
            "inception43(x)\n",
            "           self.activ_channels, kernel_size=(1, 2)) * self.botal_size = Q)\n",
            "       self.inceptions = np.zeros((-1, self.activ_Block(x)\n",
            "           return output\n",
            "\n",
            "    def step(self, X, training=True):\n",
            "        self.running_mean = 0\n",
            "          grad_Whh = np.zeros((self.n_units))\n",
            "       if les = self.batchpeater_shape\n",
            "        self.env = np.ones(opt)\n",
            "       self.pool = np.rand\n",
            "\n",
            "epoch: 1200 | loss: 0.7679\n",
            "ize = 1:\n",
            "                          self.__out in range(Layer):\n",
            "           for i in range(N):\n",
            "             env = np.zeros((self.n_filters, int()\n",
            "        self.get_fc = nn.MaxPool2d(3, 3),\n",
            "             grad = nv.frames()\n",
            "         self.Whh = None\n",
            " \n",
            "          self.Whh = channels, intermediate_cont(self.stride, poit_channels=12, t5tumestate_channels, intermediate_channels)\n",
            "\n",
            "          Q[i] = input_shape\n",
            "          self.frame = gym.Ost3(np.viueque(t)\n",
            "          return self.memory.batch.copy(opt)\n",
            "         s\n",
            "\n",
            "epoch: 1300 | loss: 0.7372\n",
            "intermediate_channels=1adke=1)\n",
            "                 self.conv = paddend(X, axis=0)\n",
            "          self.maxpool1 = nn.incater(2092), padding=(1, 1))\n",
            "\n",
            "       self.brain = 0\n",
            "\n",
            "    def __init__(self, shape, output_shape=None):\n",
            "        nem_states = input_shape\n",
            "         print(fi, 1), channels, int_every):\n",
            "                          self.n_filtert = gym.pad(mactions)\n",
            "             self.env = action += self.output_shape(self.batck_channels, int(out_channels=rev2))\n",
            "\n",
            "        self.conv = np.copy(opt)\n",
            "        self.gamma\n",
            "\n",
            "epoch: 1400 | loss: 0.6927\n",
            "in_channels=1, kernel_size=(1, 1)), self.gamma, target_update_freward, dinputs)\n",
            "       return (1 - self.trainable):\n",
            "         if t = X, t, d += 5).to(device)\n",
            "        self.input_shape = np.zeros((batch_size)\n",
            "        rath.sqrt(nn.Module):\n",
            "    def backward_propagation(self):\n",
            "         self.memory.b = np.zeros(self.X, n_channels, env)\n",
            "\n",
            "        self.branch1 = nn.Linear(120, 0\n",
            "                    if m_shape\n",
            "        self.input_shape = np.sum(grad)\n",
            "        self.branch1(t, d - np.branch(x), self.maWxpool, s\n",
            "\n",
            "epoch: 1500 | loss: 0.6616\n",
            "it_env = X\n",
            "       self.trainable:\n",
            "                                   self.poput = self.epopulation\n",
            "\n",
            "        grad_b = np.arameturnel_size=(\n",
            "                                                                                                                                                                                                                                 state_shape = copy.copy(opt)\n",
            "         self.conv1 = reward_nput.step(input_shape[1]e, output_shape=self.pad_opt.update(self.populag):\n",
            "    \n",
            "\n",
            "epoch: 1600 | loss: 0.6592\n",
            "ict(self.n_units,))\n",
            "\n",
            "                    for s in range(self.gamma, target_upads):\n",
            "        grad_Why = self.layer_input hals = 2\n",
            "env.size (1 - 1)\n",
            "        self.pool_size=\":\n",
            "                         self.filter_shape, revers, input_size *xape)\n",
            "\n",
            "    def __init__(self, env)\n",
            "\n",
            "    def observation = self.input_shape\n",
            "                             self.conv(x)\n",
            "                        grad__ = np.zeros([i]\n",
            "                           _, in in reward\n",
            "                        pacding_start = self.output_shape[0]\n",
            "\n",
            "epoch: 1700 | loss: 0.5978\n",
            "ilter_shape[(1, 1))\n",
            "       self.conv = nn.Linear(512 * 516\n",
            "\n",
            "        grad_Wxh = np.argmax(self.memory(self):\n",
            "             if a pool = nn.MaxPool2d(kernel_size=2)\n",
            "       return np.prod(self.Whh, grad_]:\n",
            "        self.Wx = np.zeros_like(n_wim, pool_shape=None:\n",
            "                   if self.target_net(nn.Module):\n",
            "    def __init__(self, env) * prev_h = np.zeros((N, self.n_units))\n",
            "               next_c = np.zeros((N, self.n_units))\n",
            "               self.X_norm = X_net.state(self.Wxh, grad_Wxh, grad_Wx, grad_\n",
            "\n",
            "epoch: 1800 | loss: 0.6069\n",
            "inputs, shape=None):\n",
            "        self.sev_c = np.zeros((self.prev_shape)\n",
            "\n",
            "         self.pool_backward()\n",
            "        self.shape = None\n",
            "        self.conv2 = nn.MaxPool2D(Layer):\n",
            "    def __init__(self, in_channels += 1):\n",
            "        return action = torch.dim * self.input_shape\n",
            "        x = self.gamma_opt.update(self.Whh) * fh)\n",
            "        x = self.input_shape\n",
            "        for t in range(g_maxlang)\n",
            "        self.branch = nn.Conv2d(self.n_action))\n",
            "        self.shape = np.zeros((N, N, self.padding)\n",
            "\n",
            "         self.stride = 1\n",
            "\n",
            "\n",
            "epoch: 1900 | loss: 0.5809\n",
            "ing=True):\n",
            "        self.dh_prev = np.zeros((len(replay), self.n_units))\n",
            "         layers += yp.reshape(self.memory), self.memory, backwad_masma[normentutensor)\n",
            "\n",
            "\n",
            "class Actiut_channels * env.reseted()\n",
            "\n",
            "\n",
            "class FalsedNut(next_channels=Setward):\n",
            "            np.prod(self.b = np.zeros((N, self.n_units))\n",
            "           w_out in range(X[:, t, :]))\n",
            "         self.population = self.relu(self.steps, input_dim))\n",
            "        self.relu = nn.Conv2d(in_channels=81, 1)00\n",
            "        self.Wh = np.zeros((self.n_units))\n",
            "        \"\n",
            "\n",
            "epoch: 2000 | loss: 0.5243\n",
            "idth_factor, axis=0)\n",
            "\n",
            "        self.b_opt = copy.copy(opt)\n",
            "\n",
            "    def n_pramess(filter_shape, input_shape=(2, 2)),\n",
            "                                                                                                                                                                                                                                                                                                                                                                       for Farget_vals * \\\n",
            "          \n",
            "\n",
            "epoch: 2100 | loss: 0.5436\n",
            "ide_deuredict_actions img_size = storch.nn.resize(self.w,) + self.stride, self.input_shape))\n",
            "\n",
            "    def buing_state_statt, dop, pred_c, -1, 24)\n",
            "    x = stride + np.prod(self.n_filters, self.n_actions))\n",
            "        baby1.layers[i].copy()\n",
            "        self.b = np.zeros((N, self.n_units))\n",
            "        if np.random.rand()\n",
            "        baby0.layers[i]\n",
            "                    action_batch = non_meinetermediatyer[i]\n",
            "\n",
            "    def __init__(self):\n",
            "        self.capend(x) + np.sqrt(version_channels=124, kernel_size=(\n",
            "                   \n",
            "\n",
            "epoch: 2200 | loss: 0.4792\n",
            "it_population(self, X, training=True):\n",
            "        self.cache = []\n",
            "            self.h_ba = np.mean(baby0, aw)\n",
            "\n",
            "\n",
            "device = torch.Floans!(dinters\n",
            "\n",
            "    def __init__(self, env, kwrelu': (1, 1).reshape() + memormal_probs = F.sqrt(input_shape) * self.epsilon:\n",
            "                                         i = 1 / math.sqrt(self.n_units + self.sigmoid(o)\n",
            "            if rent(self.input_shape\n",
            "               baby0.layers[i].[:, z:] = parent1.layers[i].parent1(-1)\n",
            "\n",
            "            for t in range(batch_size, stride, output\n",
            "\n",
            "epoch: 2300 | loss: 0.5366\n",
            "ins in range(F):\n",
            "                                           x = x.reshape(self.b, grad_Why)\n",
            "            parent1 = np.mean(X.shape) + np.prod(self.h)\n",
            "                self.gamma = np.zeros_like(X)\n",
            "        self.maxpool1 = nn.BatchNorm2d(output_shape)\n",
            "\n",
            "        output = np.zeros_like(env.sum(x)\n",
            "        return np.prod(self.Why) + \\\n",
            "            Q[i][a] += self.conv1(x)\n",
            "        for i in range(len(baby0.layers))\n",
            "\n",
            "    def backward_propagation(self, X, training=True):\n",
            "        return self.in_channels\n",
            "\n",
            "      \n",
            "\n",
            "epoch: 2400 | loss: 0.4518\n",
            "igmoid(h), pwidth, self.padding)\n",
            "        total_reward = 0\n",
            "            grad_b = np.zeros((N, i, f, h, grad_Whh)\n",
            "        self.beta = np.sum(grad, axis=0)\n",
            "        self.db = np.ardapte(self.beta_opt)\n",
            "\n",
            "        a = gate_f * self.e_start\n",
            "        self.init_parameters(self):\n",
            "        return np.prod(self.Wh.shape) == 3:\n",
            "            Q[i][a] += self.b_opt\n",
            "        if len(parent[0]\n",
            "        self.cache + in reward_propagation(self, grad):\n",
            "        total_reward += True:\n",
            "            parentar = np.sum(grad_Whh, grad_\n",
            "\n",
            "epoch: 2500 | loss: 0.4562\n",
            "if gate_s\n",
            "\n",
            "        self.stride + 1\n",
            "        self.b = np.zeros((N, self.n_units))\n",
            "        out_channels = self.stddev_inv * (ph25, stride=4)\n",
            "        self.eps = X\n",
            "        self.maxpool = nn.Squeeliate(self.state_, dinputs)\n",
            "\n",
            "        self.observation_space = torch.cat(\n",
            "                                   self.pool_shape = self._population as np.dot(dinputs, self.Wh)\n",
            "        self.running_mean = np.arannge(out_channels):\n",
            "        self.std = np.arange(gener(), padding=1)\n",
            "        self.dWh = np.zeros(dept_coll\n",
            "\n",
            "epoch: 2600 | loss: 0.4587\n",
            "if self.e_steps - self.padding)\n",
            "\n",
            "        self.batchnorm4 = nn.Conv2d(in_channels)\n",
            "        self.e_class = None\n",
            "        self.input_shape == \"same shape\"\n",
            "        if lat_h_prepear(1, 1).reshape(-1, 1)\n",
            "        x = self.fc1(x)\n",
            "\n",
            "        x = ravel_size = None\n",
            "        if self.training:\n",
            "                return (1 / self.branch4 = nn.Conv2d(in_channels, out_channels, kernel_size=(\n",
            "                                                                                                                                 \n",
            "\n",
            "epoch: 2700 | loss: 0.4426\n",
            "in_population]), (pw), stride=1, padding=(1, 1))\n",
            "        next_h = np.zeros_like(self.pool_shape)\n",
            "        self.e_start = e_end\n",
            "        self._step_forward()\n",
            "                self.trainable = True\n",
            "\n",
            "    def get_frames(self):\n",
            "        self.skip = skip\n",
            "    def output_shape(self):\n",
            "        self.maxpool = nn.Conv2d(32, 46, 1280\n",
            "    self.cache.append(x)\n",
            "\n",
            "            x = self.relu(x.shape[1], -1)\n",
            "        x = self.input_shape\n",
            "        self.Whh_opt = copy.copy(opt)\n",
            "        self.observation_space.shape[0], 1)\n",
            "   \n",
            "\n",
            "epoch: 2800 | loss: 0.4179\n",
            "img=2.9, depth_factor, depth_factor, batch_size),\n",
            "                                                               n_neront_preds = np.zeros((N, self.n_units))\n",
            "        b = np.zeros((N, self.n_units))\n",
            "        self.Why = self.mask\n",
            "        self.gamma_opt = copy.copy(opt)\n",
            "\n",
            "    def _pool_shape()\n",
            "        self.input_shape = input_shape\n",
            "        self.w = np.random.uniform(-i, i, (self.n_units))\n",
            "        brain = self.env.reset()\n",
            "            brain = np.argmax(self.n_units)\n",
            "        self.opt = population_space\n",
            "\n",
            "\n",
            "\n",
            "epoch: 2900 | loss: 0.3925\n",
            "is=0)\n",
            "        self.batchnorm3 = nn.MaxPool2d(kernel_size=(\n",
            "                env.render()\n",
            "\n",
            "        self.batchnorm = nn.MaxPool2d(kernel_size=(\n",
            "                [6, 192, 2, 1, 3],\n",
            "            [6, 32, 3, 3],\n",
            "            [6, 2, 5, 5, 5],\n",
            "            [6, 32, 2, 3],\n",
            "                \"b6\": (4, 32, 0.3),\n",
            "           [6, 48, 3, 6, 10, 3],\n",
            "         [6, 196, 3, 2, 1], 512, 512, 512, 512, 512, 512, 512, 512, 512, \"M\", 5],\n",
            "            66, 320, 5],\n",
            "           [6, 48, 3, 2, 24, 64, 648, 128, 198, 128, 128, 128, 16,\n",
            "\n",
            "epoch: 3000 | loss: 0.3998\n",
            "it_channels, rad_Wh)\n",
            "        grad = grad_h.dot(self.Wxh.T)\n",
            "                self.observation_space = g\n",
            "\n",
            "            grad_Wxh = np.dot(self.Wout.T)\n",
            "            grad_Whh += self.output_shape()\n",
            "                self.total_height, width = (height - self.pool_shape[0], -1)\n",
            "        return nn.Sequential(\n",
            "                        intermediate_channels, hidden_dim, kernel_size=3, stride=1\n",
            "            grad_Why += self.Wout_opt.update(self.Whh)\n",
            "\n",
            "        self.trainable = True\n",
            "            self.batchnorm2 = nn.Co\n",
            "\n",
            "epoch: 3100 | loss: 0.4017\n",
            "int(self.n_filters, c, filter_shape, output_shape=self.cache[grad, (batch_size)\n",
            "epochs = (1 - self.pool_shape[1])\n",
            "        for episode is Layer(object):\n",
            "            self.b = np.zeros(d\n",
            "                self.conv3 = nn.Conv2d(intermediate_channels, out_channels=156, 10),\n",
            "                                     intermediate_chancelisteps, depcation1, dmaxpool1(x)\n",
            "\n",
            "        x = self.inception3a(x)\n",
            "        x = self.inception3a(x)\n",
            "        x = x.view(x.shape[0], -1)\n",
            "\n",
            "        self.target_update = gym.spaces.B\n",
            "\n",
            "epoch: 3200 | loss: 0.3912\n",
            "input_shape(self):\n",
            "        return output\n",
            "\n",
            "    def n_parameters(self):\n",
            "        return grad * self.h_next\n",
            "\n",
            "        self.conv3 = nn.Conv2d(in_channels)\n",
            "        self.print_every = np.zeros((batch_size, timesteps, input_dim))\n",
            "\n",
            "        if ns in range(N):\n",
            "                    grad_b = np.zeros((batch_size, timesteps, padding=(0, 0))\n",
            "        self.render = self.env.step(a)\n",
            "\n",
            "        return grad * self.mask\n",
            "\n",
            "    def output_shape(sels):\n",
            "        self.env = nn.Linear(7 * np.random.uniform(-i, i, (self.n_units, \n",
            "\n",
            "epoch: 3300 | loss: 0.3652\n",
            "images, filter_shape, dtrchning=kend, self.n_units))\n",
            "        self.eps = np.tile(np.arange(ow), fh * fw = self.padding)\n",
            "\n",
            "        self.opt.step()\n",
            "        self.apd = X[:, t].dot(self.Wx.T)\n",
            "        self.b = self.b_opt.update(self.gamma, grad_gamma)\n",
            "\n",
            "    def forward_propagation(self, X, training=True):\n",
            "        self.batchnorm1 = nn.BatchNorm2d(intermediate_channels=16, kernel_size=(\n",
            "                                                  for w_out[i] * self.padding) * torch.cuda.dima(lose, self.padding)\n",
            "    \n",
            "\n",
            "epoch: 3400 | loss: 0.3633\n",
            "import images\n",
            "        self.Why = None\n",
            "        self.batchnorm = nn.BatchNorm2d(64)\n",
            "        self.relu = nn.Dropout(Layer):\n",
            "    def h_backward(self, step(self):\n",
            "        return (fh, b) = 1 / mak\n",
            "            self.Wh = None\n",
            "        self.w = np.zeros((N, T, self.n_units))\n",
            "        output = None\n",
            "        self.batchnorm3 = nn.BatchNorm2d(\n",
            "                                                  self.h_prev = np.random.uniform(-i, i, (self.n_units))\n",
            "        self.Wxh = np.random.uniform(-i, i, (self.in_channels, ker\n",
            "\n",
            "epoch: 3500 | loss: 0.3482\n",
            "ing)\n",
            "        self.Wxh = None\n",
            "        self.frames = grad.reshape(self.grad_next, self.Wx.T)\n",
            "        self.layer_input = X\n",
            "        grad_h = grad_h * prev_c\n",
            "            c, h, w = self.fc1(x)\n",
            "        x = self.momentum * \\\n",
            "                                                                                                                                                                                                                                                                             self.running_me\n",
            "\n",
            "epoch: 3600 | loss: 0.3675\n",
            "ize),  = 4, 4096),\n",
            "                 nn.ReLU(),\n",
            "                                            nn.ReLU(),\n",
            "                         nn.Dropout(p=0.7)\n",
            "        self.aux2 = AuxilaryBlock(cif, fw) / self.stddev_inv * (batch_size * channels * width_factor) \n",
            "        self.filter_shape = pool_shape\n",
            "        if len(self.layer_input.shape)\n",
            "\n",
            "        return output\n",
            "\n",
            "    def __len(self, version, axis=1)\n",
            "        self.beta = next_h * np.tanh(new_state, done):\n",
            "        self.learning_start, e_start, e_end, expand_ratio=e\n",
            "\n",
            "epoch: 3700 | loss: 0.3406\n",
            "inputs = np.uinater(\n",
            "            InvertedResidualBlock(in_channels, out_channels, kernel_size=1, bias=False),\n",
            "               stride=self.stride, 1)\n",
            "        self.a_fc2 = nn.Linear(1024, 10000\n",
            "print_every =5, 5280\n",
            "ens in self.target_net)\n",
            "        self.Whh = self.Wx_opt.update(self.gamma, grad_gamma)\n",
            "        self.X_norm = X_centered * self.steps > self.e_end = X.shape\n",
            "        self.block2 = self.get_layers(\n",
            "            dnext_h = np.dot(self.layer_input.shape)\n",
            "\n",
            "        c = self.batchnorm1(x)\n",
            "        x \n",
            "\n",
            "epoch: 3800 | loss: 0.3431\n",
            "init__(self, env, k=1, stride=1, padding=0)\n",
            "        self.branch2 = nn.Sequential(nn.Conv2d(self.in_channels, 1, height, width)\n",
            "\n",
            "        self.Why = self.b_opt = copy.copy(opt)\n",
            "\n",
            "    def nstep(self):\n",
            "        self.Why = self.pop_size - 1)\n",
            "\n",
            "    def select_start:\n",
            "        X_con = self.frames\n",
            "        self.trainable = True\n",
            "        self.frames = (h + np.sum(ph) / self.stride + 1\n",
            "        output = self.Wx_opt.update(self.Wx, grad_Wout)\n",
            "        if output = np.zeros(self.filter_shape):\n",
            "    env = 1 / \\\n",
            "        \n",
            "\n",
            "epoch: 3900 | loss: 0.3351\n",
            "ing=True):\n",
            "        self.trainable = True\n",
            "\n",
            "    def output_shape(self):\n",
            "        return np.prod(self.Wxe.shape) + np.prod(self.b.shape)\n",
            "\n",
            "    def output_shape(self):\n",
            "        for h_out in range(H_out):\n",
            "            intermediate_channels=125, out_channels):\n",
            "        if self.training=True):\n",
            "        self.frames = torch.veqs(self.input_shape)\n",
            "\n",
            "        return output\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = self.inception4d(x)\n",
            "            return self.input_shape\n",
            "\n",
            "\n",
            "class WrapFrame(gym.RewardWrapper.__init__(self\n",
            "\n",
            "epoch: 4000 | loss: 0.3139\n",
            "ing)\n",
            "        self.batchnorm = nn.BatchNorm2d(out)\n",
            "\n",
            "    def n_parameters(self, opt):\n",
            "        self.gamma = np.sum(grad[0, out_channels)\n",
            "        self.X_norm = X_centered * self.stddev_inv * (batch_size * grad.T - np.sum(grad, axis=0) -\n",
            "                                                [6, 160, 16, 0, 1, 3],\n",
            "            nn.ReLU(),\n",
            "                       nn.ReLU(),\n",
            "                                                nn.Sigmoid,\n",
            "                                   nn.ReLU(),\n",
            "                     mem_rewards =\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-62b7703f88b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-5b4df76b1db2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, chunk_len, lr, show_every)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mchunk_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFzleAEqtRMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8661b09-677d-4bae-a830-fb0293126744"
      },
      "source": [
        "prime = \"import numpy as np\"\n",
        "print(net.generate(prime, 5000, 0.75))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "import numpy as np\n",
            "import Image\n",
            "import random\n",
            "import np\n",
            "import numpy\n",
            "import copy\n",
            "import imagensiforms(frame, conv)\n",
            "\n",
            "    def stack_memory(self, activ_fn):\n",
            "        self.running_var = None\n",
            "        return grad * self.e_end * self.stride\n",
            "                                                                                                                                                                                                                                                                                                                                                      size=layer.b.shape)\n",
            "\n",
            "    def forward_propagation(self, X, training=True):\n",
            "        self.padding == 3:\n",
            "                self.stride = stride\n",
            "        self.n_filters = n_filters\n",
            "        self.layer_input = X\n",
            "        self.beta_opt = copy.copy(opt)\n",
            "\n",
            "    def forward_propagation(self, X, training=True):\n",
            "        self.mask = None\n",
            "\n",
            "    def __init__(self, input_shape=None):\n",
            "        self.Whh = None\n",
            "        self.layer_input = X\n",
            "        grad_Wout = np.sum(grad, axis=0).flatten()\n",
            "        output = output.reshape(self.layer_input.shape)\n",
            "\n",
            "    def forward_propagation(self, X, training=True):\n",
            "        self.trunc = trunc\n",
            "        self.Why_opt = copy.copy(opt)\n",
            "        self.Why_opt = copy.copy(opt)\n",
            "\n",
            "    def forward_propagation(self, X, training=True):\n",
            "        self.trainable = True\n",
            "        self.w = self.w_opt.update(self.X_lat.T).reshape(self.beta_opt.update(self.b, grad_b)\n",
            "            self.layer_input = X\n",
            "        output = X_col.shape[0]\n",
            "                self.mask = None\n",
            "        self.output = None\n",
            "        self.Wx = None\n",
            "        self.b_opt = copy.copy(opt)\n",
            "\n",
            "    def n_parameters(self):\n",
            "        return x\n",
            "        else:\n",
            "                                        self.trainable = True\n",
            "\n",
            "    def forward_propagation(self, X, training=True):\n",
            "        self.frames = None\n",
            "\n",
            "    def init_parameters(self, opt):\n",
            "            self.memory, a = self.get_layers(\n",
            "                3000lentity_state_batch)\n",
            "\n",
            "        if self.trainable:\n",
            "                X_pad[n, :, height:height+FH, width:width +\n",
            "               F, H_out, W_out = self.get_output())\n",
            "        x = self.Wxh_opt.update(self.Wout, grad_Wout)\n",
            "        return (ph1, ph2), (pw2, prev_c, i, f, out):\n",
            "                                                                                                                                                                                                                                                                  in_channels == Madh\n",
            "        epsilon = self.get_epsilon()\n",
            "            baby0.layers[i].w[:, z:] = parent1.layers[i].w[:, z:] = parents[i]\n",
            "\n",
            "        return transform * np.exp(-self.memory, bs)\n",
            "\n",
            "        return output\n",
            "\n",
            "    def _pool_backward(self, accum_grad):\n",
            "        if self.frames = self.epsilon\n",
            "            if done:\n",
            "                         if net_preds = self.pool_shape\n",
            "        self.prev_shape = None\n",
            "        self.Wx = None\n",
            "             if self.trainable:\n",
            "                    accum_grad_col[shape = input_shape\n",
            "        self.gamma = self.get_layers(\n",
            "            architecture[2], intermediate_channels=kernel_size=(\n",
            "            = env.observation(self, input_shape=None):\n",
            "                    accum_grad_col.shape[0] * accum_grad_col[arg_max, range(accum_grad.size))\n",
            "        self.inception4c = InceptionBlock(480, 192, :24)\n",
            "        self.activ_fn = random.random() < self.stride = 1:\n",
            "            state = self.epsilon:\n",
            "                if trainable:\n",
            "                for t in range(T):\n",
            "            dgate_f = np.zeros((N, T, self.n_units))\n",
            "        self.b = np.sum(dh, axis=1)\n",
            "\n",
            "    def forward_propagation(self, X, training=True):\n",
            "        self.gamma = self.w_opt.update(self.Why, grad_b)\n",
            "        self.gamma_opt = copy.copy(opt)\n",
            "\n",
            "    def n_parameters(self):\n",
            "                        net_actions = net(state)\n",
            "\n",
            "                        action = self.pool_shape\n",
            "        self.w = np.zeros((self.n_filters))\n",
            "        self.b = self.get_output())\n",
            "        return output\n",
            "\n",
            "    def backward_propagation(self, grad):\n",
            "        return grad\n",
            "\n",
            "    def output_shape(self):\n",
            "        return X * (1 - self.pool_shape)\n",
            "\n",
            "    def forward_propagation(self, X, training=True):\n",
            "        self.Whh = None\n",
            "        self.Wxh = None\n",
            "        self.trainable = True\n",
            "        self.b = self.b_opt.update(self.Wout, grad_Wout)\n",
            "        self.Wxh = None\n",
            "        self.gamma_opt = copy.copy(opt)\n",
            "\n",
            "    def forward_propagation(self, X, training=True):\n",
            "        height, width = h_out * self.stride, w_out * self.stride\n",
            "                                                                                                                                                                                                                       architecture[1], intermediate_channels=128, stride=2)\n",
            "        self.layer_input = X\n",
            "        self.Wxh_opt = copy.copy(opt)\n",
            "\n",
            "    def n_parameters(self):\n",
            "        return np.prod(np.shape(self.b))\n",
            "\n",
            "    def forward_propagation(self, X, training=True):\n",
            "            self.brain_fn = self.Transition(\n",
            "            1 - self.render == 0\n",
            "            action = baby.activ_fn.beta)\n",
            "\n",
            "        self.pool_shape = pool_shape\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlJ77aONgf-H"
      },
      "source": [
        "# \\_()_/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}